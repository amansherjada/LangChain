{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Vector Search**\n",
        "### What is Vector Search?\n",
        "\n",
        "Vector search is a method of finding similar items by comparing their vector representations. In the context of AI and machine learning, items such as words, images, or documents are often represented as vectors (arrays of numbers). These vectors capture the essential features or meanings of the items in a mathematical form.\n",
        "\n",
        "### Why Use Vector Search?\n",
        "\n",
        "Traditional search methods rely on exact matches of keywords, which might not always capture the true similarity between items. Vector search, on the other hand, can find similar items even if they don't share the exact same words or features, making it powerful for tasks like recommendation systems, image search, and natural language processing.\n",
        "\n",
        "### How Does Vector Search Work?\n",
        "\n",
        "1. **Convert Items to Vectors:**\n",
        "   - Each item (e.g., a word, sentence, or image) is converted into a vector. This can be done using techniques like word embeddings for text or deep learning models for images.\n",
        "\n",
        "2. **Store Vectors in a Database:**\n",
        "   - All the vectors are stored in a database. This database can be searched efficiently to find similar vectors.\n",
        "\n",
        "3. **Search for Similar Vectors:**\n",
        "   - When a query vector (representing the item you're searching for) is provided, the system finds the vectors in the database that are closest to the query vector. This \"closeness\" is usually measured using a distance metric like cosine similarity or Euclidean distance.\n",
        "\n",
        "### Simple Example\n",
        "\n",
        "Let's say we have a vector search system for finding similar movies based on their descriptions.\n",
        "\n",
        "1. **Convert Movie Descriptions to Vectors:**\n",
        "   - We use a pre-trained language model to convert each movie description into a vector.\n",
        "     - \"A young wizard attends a magical school.\" -> [0.1, 0.3, 0.4, ..., 0.2]\n",
        "     - \"A group of friends embarks on a quest to destroy a powerful ring.\" -> [0.2, 0.1, 0.3, ..., 0.4]\n",
        "\n",
        "2. **Store Vectors:**\n",
        "   - Store these vectors in a database.\n",
        "\n",
        "3. **Search with a Query:**\n",
        "   - A user searches for a movie similar to \"A boy discovers he is a wizard and goes on adventures.\"\n",
        "   - Convert this query into a vector: [0.1, 0.2, 0.4, ..., 0.3]\n",
        "   - The system compares this query vector with the vectors in the database and finds the closest matches.\n",
        "   - It might find that \"A young wizard attends a magical school.\" is the closest match.\n",
        "\n",
        "### Benefits of Vector Search\n",
        "\n",
        "- **Semantic Search:** Finds similar items based on meaning, not just keywords.\n",
        "- **Flexibility:** Works with different types of data (text, images, etc.).\n",
        "- **Accuracy:** Can capture subtle similarities and nuances in data.\n",
        "\n",
        "### Summary\n",
        "\n",
        "Vector search is a powerful technique that finds similar items by comparing their vector representations. It converts items into vectors, stores these vectors, and uses distance metrics to find the closest matches. This method is widely used in AI applications for its ability to understand and capture the underlying similarities between items."
      ],
      "metadata": {
        "id": "ldaPwlCxwswg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Cassandra and Astra**\n",
        "\n",
        "Cassandra and Astra are both databases, but they have different characteristics and use cases. Here's an explanation of each:\n",
        "\n",
        "### Cassandra Database\n",
        "\n",
        "**Apache Cassandra** is an open-source, distributed NoSQL database designed to handle large amounts of data across many commodity servers, providing high availability with no single point of failure. It's known for its scalability and performance, making it suitable for applications that require fast and reliable data access.\n",
        "\n",
        "**Key Features of Cassandra:**\n",
        "\n",
        "1. **Distributed Architecture:** Data is distributed across multiple nodes in a cluster, ensuring fault tolerance and high availability.\n",
        "2. **Scalability:** Easily scales horizontally by adding more nodes to the cluster without downtime.\n",
        "3. **No Single Point of Failure:** Data is replicated across multiple nodes, so the failure of one node doesn't affect the system's overall availability.\n",
        "4. **High Performance:** Designed for high-speed read and write operations, making it suitable for applications that need to handle large volumes of data quickly.\n",
        "5. **Flexible Schema:** Allows for a flexible, schema-less design, enabling dynamic and unstructured data models.\n",
        "6. **Query Language:** Uses CQL (Cassandra Query Language), which is similar to SQL but tailored for Cassandra's distributed architecture.\n",
        "\n",
        "**Use Cases for Cassandra:**\n",
        "- Real-time big data applications\n",
        "- High-velocity data ingestion\n",
        "- Distributed data stores for web applications\n",
        "- IoT data management\n",
        "- Financial services for transaction data\n",
        "\n",
        "### Astra Database\n",
        "\n",
        "**Astra DB** is a database-as-a-service (DBaaS) offering built on Apache Cassandra by DataStax. It provides a managed, cloud-native version of Cassandra, making it easier to deploy, manage, and scale Cassandra databases without the complexity of manual administration.\n",
        "\n",
        "**Key Features of Astra:**\n",
        "\n",
        "1. **Managed Service:** Astra DB takes care of the database management tasks such as provisioning, maintenance, scaling, and backups, allowing developers to focus on building applications.\n",
        "2. **Cloud-Native:** Designed to run on major cloud platforms (AWS, Google Cloud, Azure), providing flexibility and integration with cloud services.\n",
        "3. **Serverless Architecture:** Supports a serverless model, where users only pay for the actual usage (reads/writes), providing cost efficiency.\n",
        "4. **Easy to Use:** Provides a simplified interface and developer-friendly tools, including SDKs, REST APIs, and GraphQL support.\n",
        "5. **Global Distribution:** Allows for the deployment of globally distributed databases with low-latency access to data across multiple regions.\n",
        "6. **Compatibility with Cassandra:** Fully compatible with Cassandra, allowing for easy migration and integration with existing Cassandra-based applications.\n",
        "\n",
        "**Use Cases for Astra:**\n",
        "- Modern web and mobile applications\n",
        "- Real-time analytics and big data applications\n",
        "- Applications requiring high availability and global distribution\n",
        "- Enterprises looking for a scalable and managed database solution without the operational overhead\n",
        "\n",
        "### Summary\n",
        "\n",
        "- **Apache Cassandra**: An open-source, distributed NoSQL database known for its scalability, high performance, and fault tolerance. Ideal for applications needing to handle large amounts of data with high availability.\n",
        "- **Astra DB**: A managed, cloud-native version of Cassandra provided by DataStax. Simplifies the deployment and management of Cassandra databases, offering a serverless architecture, global distribution, and seamless integration with cloud services.\n",
        "\n",
        "Both databases are powerful tools for managing large-scale data, but Astra DB simplifies many of the operational complexities associated with managing a Cassandra cluster, making it an attractive choice for developers and enterprises looking for a managed solution.\n",
        "\n",
        "Cassandra = https://cassandra.apache.org/doc/latest/\n",
        "\n",
        "Astra = https://www.datastax.com/products/datastax-astra"
      ],
      "metadata": {
        "id": "ZLUQyPMexhG2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxW6KmXZuJ8-",
        "outputId": "928aa80a-12ea-487a-df98-dfb8c3d5143e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m886.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.6/974.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.4/327.4 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.9/18.9 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.8/321.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.4/127.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q cassio datasets langchain tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Library Details**\n",
        "\n",
        "### 1. cassio\n",
        "\n",
        "**CassIO** is a library designed for integration with Apache Cassandra, making it easier to use for AI and ML applications. It often provides utilities for handling data ingestion, storage, and retrieval in a way that leverages Cassandra's distributed architecture.\n",
        "\n",
        "**Key Features:**\n",
        "- Simplifies interaction with Cassandra databases.\n",
        "- Provides tools for managing AI/ML data workflows.\n",
        "- Helps in efficiently storing and querying large datasets.\n",
        "\n",
        "### 2. datasets\n",
        "\n",
        "**Datasets** is a library developed by Hugging Face. It's designed to provide easy access to a wide variety of datasets for machine learning and data science projects.\n",
        "\n",
        "**Key Features:**\n",
        "- Access to a large collection of datasets for NLP, computer vision, and more.\n",
        "- Easy-to-use API for downloading, preprocessing, and managing datasets.\n",
        "- Integration with popular ML frameworks like PyTorch, TensorFlow, and Hugging Face's Transformers.\n",
        "\n",
        "### 3. langchain\n",
        "\n",
        "**LangChain** is a framework for building applications with large language models (LLMs). It is particularly focused on enabling applications that are data-aware and can connect to external sources of data.\n",
        "\n",
        "**Key Features:**\n",
        "- Framework for developing complex applications using LLMs.\n",
        "- Provides tools for data connection, such as APIs and databases.\n",
        "- Facilitates the creation of applications that can process and generate human-like text based on data from multiple sources.\n",
        "\n",
        "### 4. tiktoken\n",
        "\n",
        "**Tiktoken** is a library used for tokenizing text specifically for use with OpenAI's models. Tokenization is the process of converting text into tokens (which could be words, subwords, or characters) that the models can process.\n",
        "\n",
        "**Key Features:**\n",
        "- Efficiently tokenizes text for OpenAI's models.\n",
        "- Supports different encoding schemes used by OpenAI models.\n",
        "- Helps in managing token limits and optimizing input text for model consumption.\n",
        "\n",
        "### Summary\n",
        "\n",
        "- **CassIO**: Simplifies interaction with Apache Cassandra for AI/ML applications.\n",
        "- **Datasets**: Provides access to a variety of datasets for machine learning.\n",
        "- **LangChain**: Framework for building applications with large language models, especially those that need to connect to external data sources.\n",
        "- **OpenAI**: Library for accessing and using OpenAI's AI models like GPT-3 and GPT-4.\n",
        "- **Tiktoken**: Tokenization library optimized for preparing text input for OpenAI's language models.\n",
        "\n",
        "These libraries collectively cover a wide range of functionalities, from data handling and preparation to building sophisticated AI-powered applications."
      ],
      "metadata": {
        "id": "s4dM4ydF05xM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain-fireworks PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsByCrvx4d__",
        "outputId": "74d96d10-5b97-4c9b-f2bf-912f9c721f68"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.8/81.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCvFGypN5k4L",
        "outputId": "24071070-cc78-45cc-a575-32bf9ce4cf50"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.2.5-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.9.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.5)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.9)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.82)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.5->langchain-community) (0.2.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.5->langchain-community) (2.7.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.7->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.7->langchain-community) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.6.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.7->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.5->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.5->langchain-community) (2.18.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain-community-0.2.5 marshmallow-3.21.3 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores.cassandra import Cassandra\n",
        "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
        "from langchain.llms import Fireworks\n",
        "from langchain_fireworks import FireworksEmbeddings\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "import cassio"
      ],
      "metadata": {
        "id": "s3Xu1S7d05AI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader"
      ],
      "metadata": {
        "id": "wvcyxqUZ5fXl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "LQs7Vi-Y6AK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "ASTRA_DB_ID = userdata.get('db_id')\n",
        "ASTRA_DB_TOKEN = userdata.get('db_token')\n",
        "FIREWORK_API_KEY = userdata.get('fireworkapi')"
      ],
      "metadata": {
        "id": "EttoE0L05_up"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Provide the path to pdf file\n",
        "pdfreader = PdfReader(\"budget_speech.pdf\")"
      ],
      "metadata": {
        "id": "bUrsaI_k5-Pa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Extracting Text from PDF Pages and Concatenating\n",
        "\n",
        "from typing_extensions import Concatenate\n",
        "#read text from the pdf\n",
        "raw_text = ''\n",
        "for i, page in enumerate(pdfreader.pages):\n",
        "  content = page.extract_text()\n",
        "  if content:\n",
        "    raw_text += content\n"
      ],
      "metadata": {
        "id": "ANSqV1ckRrJz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the connection to database\n",
        "\n",
        "cassio.init(token=ASTRA_DB_TOKEN, database_id=ASTRA_DB_ID)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4lqvI2EVQWr",
        "outputId": "0716565d-5fec-4deb-890a-23887aef2aea"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for 28ded3f1-49b1-4109-bba7-a8920859d872-us-east1.db.astra.datastax.com:29042:3b8afe0a-9b27-48ef-af55-d95b0aef8e45. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for 28ded3f1-49b1-4109-bba7-a8920859d872-us-east1.db.astra.datastax.com:29042:3b8afe0a-9b27-48ef-af55-d95b0aef8e45. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "ERROR:cassandra.connection:Closing connection <LibevConnection(140550921992944) 28ded3f1-49b1-4109-bba7-a8920859d872-us-east1.db.astra.datastax.com:29042:3b8afe0a-9b27-48ef-af55-d95b0aef8e45> due to protocol error: Error from server: code=000a [Protocol error] message=\"Beta version of the protocol used (5/v5-beta), but USE_BETA flag is unset\"\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 5 to 4 for 28ded3f1-49b1-4109-bba7-a8920859d872-us-east1.db.astra.datastax.com:29042:3b8afe0a-9b27-48ef-af55-d95b0aef8e45. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the LangChain embedding and LLM objects for later usage\n",
        "\n",
        "llm = Fireworks(fireworks_api_key = FIREWORK_API_KEY)\n",
        "embedding = FireworksEmbeddings(fireworks_api_key = FIREWORK_API_KEY)"
      ],
      "metadata": {
        "id": "XQhkZVq_VaPa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a LangChain vector store\n",
        "astra_vector_store = Cassandra(\n",
        "    embedding=embedding,\n",
        "    table_name=\"qa_demo\",\n",
        "    session=None,\n",
        "    keyspace=None\n",
        ")"
      ],
      "metadata": {
        "id": "ICHPSqQRWb0S"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting Text into Chunks with Character Text Splitter"
      ],
      "metadata": {
        "id": "epCI9kePZa0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "#We need to split the text using Character Text Splitter such that it should not increase token size\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator= \"\\n\",\n",
        "    chunk_size = 800,\n",
        "    chunk_overlap = 200,\n",
        "    length_function = len\n",
        ")\n",
        "\n",
        "texts = text_splitter.split_text(raw_text)"
      ],
      "metadata": {
        "id": "l8hxM7nZXfhh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts[0:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6UatEZ7Y-uK",
        "outputId": "72b2287d-76b6-465f-8827-89752c796df2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['GOVERNMENT OF INDIA\\nINTERIM BUDGET 2024-2025\\nSPEECH\\nOF\\nNIRMALA SITHARAMAN\\nMINISTER OF FINANCE\\nFebruary 1,  2024 \\nCONTENTS  \\n \\nPART – A \\n Page No.  \\nIntroduction  1 \\nInclusive Development and Growth  2 \\nSocial Justice   3  \\nExemplary  Track Record of Governance,  \\nDevelopment and Performance (GDP)  7 \\nEconomic Management  8 \\nGlobal Context  9 \\nVision for ‘Viksit Bharat’  10 \\nStrategy for  ‘Amrit Kaal’  11 \\nInfrastructure Development  17 \\nAmrit Kaal as Kartavya Kaal  22 \\nRevised Estimates 2023 -24 23 \\nBudget Estimates 2024 -25 23 \\nPART – B \\nDirect taxes  25 \\nIndirect Taxes   26 \\nEconomy – Then and Now  28 \\n  \\n  1 \\n Interim Budget 2024 -2025  \\nSpeech of  \\nNirmala Sitharaman  \\nMinister of Finance  \\nFebruary 1, 2024  \\nHon’ble Speaker,  \\n I present the Interim Budget for 2024 -25.',\n",
              " '1 \\n Interim Budget 2024 -2025  \\nSpeech of  \\nNirmala Sitharaman  \\nMinister of Finance  \\nFebruary 1, 2024  \\nHon’ble Speaker,  \\n I present the Interim Budget for 2024 -25.  \\nIntroduction  \\n1. The Indian  economy  has witnessed profound positive \\ntransformation in the last ten years. The people of India are \\nlooking ahead to the future with hope and optimism.  \\n2. With the blessings of the people, when our Government \\nunder the visionary and dynamic leadership of Hon’ble Prime \\nMinister Shri Narendra Modi assumed office in 2014, the country \\nwas facing enormous challenges. With ‘Sabka Saath, Sabka \\nVikas’  as its ‘mantra’ , the Government overcame those \\nchallenges in right earnest. Structural reforms were undertaken. \\nPro-people programmes were formulated and implemented',\n",
              " 'Vikas’  as its ‘mantra’ , the Government overcame those \\nchallenges in right earnest. Structural reforms were undertaken. \\nPro-people programmes were formulated and implemented \\npromptly. Conditions were cr eated for more opportunities for \\nemployment and entrepreneurship. The economy got a new \\nvigour. The fruits of development started reaching the people at \\nscale. The country got a new sense of purpose and hope. \\nNaturally, the people blessed the Government wi th a bigger \\nmandate.  2 \\n 3. In the second term, our Government under the leadership \\nof Hon’ble Prime Minister doubled down on its responsibilities to \\nbuild a prosperous country with comprehensive development of \\nall people and all regions. Our Government strengthened its']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the dataset into Vector Store\n",
        "\n",
        "The `VectorStoreIndexWrapper` is a wrapper class that provides additional functionality on top of the Astra Vector Store. The exact behavior of the wrapper class depends on the implementation, but here are some common features it might provide:\n",
        "\n",
        "1. **Indexing**: The wrapper might create an index on the stored vectors, allowing for efficient querying and filtering of the data.\n",
        "2. **Querying**: The wrapper might provide methods for querying the stored vectors, such as:\n",
        "\t* `search`: Find vectors similar to a given query vector.\n",
        "\t* `get_nearest_neighbors`: Retrieve the k-nearest neighbors to a given query vector.\n",
        "\t* `get_similar_vectors`: Retrieve vectors similar to a given query vector.\n",
        "3. **Filtering**: The wrapper might provide methods for filtering the stored vectors, such as:\n",
        "\t* `filter_by_distance`: Retrieve vectors within a certain distance from a given query vector.\n",
        "\t* `filter_by_similarity`: Retrieve vectors with a certain similarity score to a given query vector.\n",
        "4. **Aggregation**: The wrapper might provide methods for aggregating the stored vectors, such as:\n",
        "\t* `sum_vectors`: Compute the sum of multiple vectors.\n",
        "\t* `mean_vectors`: Compute the mean of multiple vectors.\n",
        "5. **Serialization**: The wrapper might provide methods for serializing and deserializing the stored vectors, allowing for easy storage and retrieval of the data.\n",
        "6. **Compression**: The wrapper might provide methods for compressing and decompressing the stored vectors, reducing storage requirements and improving query performance.\n",
        "\n",
        "By wrapping the Astra Vector Store with this wrapper class, you can gain additional functionality and flexibility when working with the stored vectors."
      ],
      "metadata": {
        "id": "-_xLWNSXav63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# astra_vector_store is also performing Embedding as mention in Setup\n",
        "astra_vector_store.add_texts(texts[:50])\n",
        "\n",
        "print(\"Inserted %i headlines\" %len(texts[:50]))\n",
        "\n",
        "astra_vector_store = VectorStoreIndexWrapper(vectorstore= astra_vector_store)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vn6BeP1sZBOh",
        "outputId": "c2c8d63b-00fc-4a91-d02f-cbf6db55393e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inserted 50 headlines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the QA\n",
        "\n",
        "Some suggested questions\n",
        "\n",
        "*   What is the Current GDP?\n",
        "*   How many and what caste we need to focus on?\n"
      ],
      "metadata": {
        "id": "maxXFBU-bvUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_question = True\n",
        "while True:\n",
        "  if first_question:\n",
        "    query_text  = input(\"Enter the question (or type 'quit' to exit): \").strip()\n",
        "  else:\n",
        "    query_text = input(\"What's your next question (or type 'quit' to exit): \").strip()\n",
        "  if query_text.lower() == \"quit\":\n",
        "    break\n",
        "  if query_text == \"\":\n",
        "    continue\n",
        "\n",
        "  first_question = False\n",
        "\n",
        "  print(f\"Question: {query_text}\")\n",
        "  answer = astra_vector_store.query(query_text, llm=llm).strip()\n",
        "  print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXetBknmbunw",
        "outputId": "4939b4fa-c093-4644-b7db-bd62f2344d5e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the question (or type 'quit' to exit): How many and what caste we need to focus on?\n",
            "Question: How many and what caste we need to focus on?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: The Prime Minister believes that we need to focus on four major castes - Garib (Poor), Mahilayen (Women), Yuva (Youth), and Annadata (Farmer).\n",
            "What's your next question (or type 'quit' to exit): quit\n"
          ]
        }
      ]
    }
  ]
}