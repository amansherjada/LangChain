{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# [Groq](https://groq.com/)\n\n- [Groq Api Key](https://console.groq.com/keys)\n- [Groq models](https://console.groq.com/docs/models)\n- [ChatGroq](https://api.python.langchain.com/en/latest/chat_models/langchain_groq.chat_models.ChatGroq.html)\n\n### What is Groq?\n\nGroq is a technology company that specializes in designing and developing high-performance hardware accelerators for artificial intelligence (AI) and machine learning (ML) workloads. The company's flagship product is the GroqChip, which is designed to provide unparalleled performance, scalability, and efficiency for AI and ML tasks, particularly for large language models (LLMs) and generative AI (GenAI) applications.\n\n### What is the LPU™ Inference Engine?\n\nThe LPU™ (Tensor Streaming Processor) Inference Engine is a core component of Groq's hardware architecture. It is designed to execute AI and ML inference tasks with high efficiency and speed. Unlike traditional GPU architectures, which rely on multiple cores to perform computations in parallel, the LPU™ Inference Engine uses a novel approach to achieve higher performance.\n\n### Why is it so much faster than GPUs for LLMs and GenAI?\n\nThe speed advantages of the LPU™ Inference Engine over traditional GPUs for LLMs and GenAI applications can be attributed to several key factors:\n\n1. **Dataflow Architecture**: Groq's architecture is designed to optimize data flow through the processor, reducing latency and maximizing throughput. This is particularly beneficial for the large and complex computations required by LLMs and GenAI.\n2. **Predictable Execution**: The GroqChip provides deterministic execution, which means that operations happen in a predictable manner, eliminating the overhead associated with managing multiple parallel threads.\n3. **High Utilization**: Groq's architecture ensures that the processing elements are highly utilized, reducing idle time and improving overall efficiency.\n4. **Optimized for Inference**: While GPUs are general-purpose and can handle both training and inference, the GroqChip is specifically optimized for inference tasks, allowing it to achieve higher performance in this area.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install python-dotenv","metadata":{"execution":{"iopub.status.busy":"2024-08-08T21:49:39.961800Z","iopub.execute_input":"2024-08-08T21:49:39.962187Z","iopub.status.idle":"2024-08-08T21:49:56.937125Z","shell.execute_reply.started":"2024-08-08T21:49:39.962158Z","shell.execute_reply":"2024-08-08T21:49:56.935802Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.10/site-packages (1.0.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install langchain-groq","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-08-08T21:49:56.939970Z","iopub.execute_input":"2024-08-08T21:49:56.940370Z","iopub.status.idle":"2024-08-08T21:50:15.020073Z","shell.execute_reply.started":"2024-08-08T21:49:56.940333Z","shell.execute_reply":"2024-08-08T21:50:15.018566Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting langchain-groq\n  Downloading langchain_groq-0.1.9-py3-none-any.whl.metadata (2.9 kB)\nCollecting groq<1,>=0.4.1 (from langchain-groq)\n  Downloading groq-0.9.0-py3-none-any.whl.metadata (13 kB)\nCollecting langchain-core<0.3.0,>=0.2.26 (from langchain-groq)\n  Downloading langchain_core-0.2.29-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain-groq) (4.2.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain-groq) (0.27.0)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain-groq) (2.5.3)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.0)\nRequirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain-groq) (4.9.0)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.26->langchain-groq) (6.0.1)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.26->langchain-groq) (1.33)\nCollecting langsmith<0.2.0,>=0.1.75 (from langchain-core<0.3.0,>=0.2.26->langchain-groq)\n  Downloading langsmith-0.1.98-py3-none-any.whl.metadata (13 kB)\nCollecting packaging<25,>=23.2 (from langchain-core<0.3.0,>=0.2.26->langchain-groq)\n  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.26->langchain-groq) (8.2.3)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (3.6)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (1.2.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (2024.7.4)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.5)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.26->langchain-groq) (2.4)\nCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain-groq)\n  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain-groq) (2.32.3)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (2.14.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain-groq) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain-groq) (1.26.18)\nDownloading langchain_groq-0.1.9-py3-none-any.whl (14 kB)\nDownloading groq-0.9.0-py3-none-any.whl (103 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-0.2.29-py3-none-any.whl (383 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.0/384.0 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langsmith-0.1.98-py3-none-any.whl (140 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.2/140.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-24.1-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: packaging, orjson, langsmith, groq, langchain-core, langchain-groq\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: orjson\n    Found existing installation: orjson 3.9.10\n    Uninstalling orjson-3.9.10:\n      Successfully uninstalled orjson-3.9.10\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkeras-cv 0.9.0 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\njupyterlab 4.2.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.2 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\npointpats 2.5.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nspaghetti 1.7.6 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.4.1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed groq-0.9.0 langchain-core-0.2.29 langchain-groq-0.1.9 langsmith-0.1.98 orjson-3.10.6 packaging-24.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install langchain_core","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-08-08T21:50:15.028661Z","iopub.execute_input":"2024-08-08T21:50:15.029053Z","iopub.status.idle":"2024-08-08T21:50:30.297715Z","shell.execute_reply.started":"2024-08-08T21:50:15.029011Z","shell.execute_reply":"2024-08-08T21:50:30.296081Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: langchain_core in /opt/conda/lib/python3.10/site-packages (0.2.29)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain_core) (6.0.1)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain_core) (1.33)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.75 in /opt/conda/lib/python3.10/site-packages (from langchain_core) (0.1.98)\nRequirement already satisfied: packaging<25,>=23.2 in /opt/conda/lib/python3.10/site-packages (from langchain_core) (24.1)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain_core) (2.5.3)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain_core) (8.2.3)\nRequirement already satisfied: typing-extensions>=4.7 in /opt/conda/lib/python3.10/site-packages (from langchain_core) (4.9.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain_core) (2.4)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.75->langchain_core) (3.10.6)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.75->langchain_core) (2.32.3)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain_core) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain_core) (2.14.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain_core) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain_core) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain_core) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain_core) (2024.7.4)\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nfrom dotenv import load_dotenv\nload_dotenv()\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"GROQ_API_KEY\")","metadata":{"execution":{"iopub.status.busy":"2024-08-08T21:50:30.299705Z","iopub.execute_input":"2024-08-08T21:50:30.300265Z","iopub.status.idle":"2024-08-08T21:50:30.510084Z","shell.execute_reply.started":"2024-08-08T21:50:30.300208Z","shell.execute_reply":"2024-08-08T21:50:30.508782Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from langchain_groq import ChatGroq","metadata":{"execution":{"iopub.status.busy":"2024-08-08T21:50:30.511836Z","iopub.execute_input":"2024-08-08T21:50:30.512776Z","iopub.status.idle":"2024-08-08T21:50:31.583173Z","shell.execute_reply.started":"2024-08-08T21:50:30.512729Z","shell.execute_reply":"2024-08-08T21:50:31.581874Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"llm = ChatGroq(model = \"gemma2-9b-it\", groq_api_key = secret_value_0)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T21:50:31.584701Z","iopub.execute_input":"2024-08-08T21:50:31.585211Z","iopub.status.idle":"2024-08-08T21:50:32.051027Z","shell.execute_reply.started":"2024-08-08T21:50:31.585165Z","shell.execute_reply":"2024-08-08T21:50:32.049727Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"llm","metadata":{"execution":{"iopub.status.busy":"2024-08-08T21:50:32.052427Z","iopub.execute_input":"2024-08-08T21:50:32.052842Z","iopub.status.idle":"2024-08-08T21:50:32.066929Z","shell.execute_reply.started":"2024-08-08T21:50:32.052809Z","shell.execute_reply":"2024-08-08T21:50:32.062979Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7d99f0268760>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7d99f02693f0>, model_name='gemma2-9b-it', groq_api_key=SecretStr('**********'))"},"metadata":{}}]},{"cell_type":"code","source":"from langchain_core.messages import HumanMessage, SystemMessage\n\nmessages = [\n    SystemMessage(content = \"Translate the Following from English to Hindi\"),\n    HumanMessage(content = \"Hello, How are you?\")\n]\n\nllm.invoke(messages).content","metadata":{"execution":{"iopub.status.busy":"2024-08-08T21:50:32.068684Z","iopub.execute_input":"2024-08-08T21:50:32.069152Z","iopub.status.idle":"2024-08-08T21:50:32.694188Z","shell.execute_reply.started":"2024-08-08T21:50:32.069109Z","shell.execute_reply":"2024-08-08T21:50:32.692906Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'नमस्ते, आप कैसे हैं? (Namaste, aap kaise hain?) \\n\\n\\nThis translates to \"Hello, how are you?\" in Hindi. \\n'"},"metadata":{}}]},{"cell_type":"markdown","source":"# LCEL\n\nLangChain Expression Language (LCEL) is a domain-specific language designed to simplify and streamline the creation and manipulation of chains in the LangChain framework. LangChain is a library that facilitates the construction of chains of components, such as language models, tools, and APIs, to create sophisticated applications involving natural language processing and understanding.\n\nLCEL allows users to define chains in a more readable and concise manner, making it easier to specify the sequence of operations and transformations applied to the data. This can include tasks like querying language models, processing text, integrating with external APIs, and more.\n\n### Key Features of LCEL\n\n1. **Readable Syntax**: LCEL provides a syntax that is easy to read and write, reducing the complexity of creating chains.\n2. **Modularity**: It promotes the modular design of components, allowing users to reuse and combine different parts of their chains.\n3. **Integration**: LCEL seamlessly integrates with LangChain's existing components and tools, making it easier to leverage the full power of the framework.\n4. **Flexibility**: Users can define complex logic and transformations using LCEL, enabling the creation of sophisticated workflows and applications.\n\n### Example Usage\n\nHere's a simple example to illustrate how LCEL might be used:\n\n```lcel\n# Define a chain that queries a language model and processes the response\n\nchain {\n  input_text = \"What is the capital of France?\"\n  \n  # Step 1: Query the language model\n  response = query_language_model(model=\"llama3\", input=input_text)\n  \n  # Step 2: Extract the answer from the response\n  answer = extract_answer(response)\n  \n  # Step 3: Output the answer\n  output(answer)\n}\n```\n\nIn this example, LCEL is used to define a chain with three steps: querying a language model, extracting an answer from the response, and outputting the answer. The syntax is designed to be straightforward, making it easier to understand and maintain the chain.\n\n### Benefits\n\n- **Ease of Use**: LCEL reduces the learning curve for new users of the LangChain framework.\n- **Efficiency**: By providing a clear and concise way to define chains, LCEL helps users save time and effort.\n- **Maintainability**: The modular and readable nature of LCEL makes it easier to maintain and update chains over time.\n\nLCEL is a powerful tool within the LangChain ecosystem, enhancing the usability and flexibility of the framework for creating advanced natural language processing applications.","metadata":{}},{"cell_type":"markdown","source":"# Building Chatbot With Message History Using Langchain","metadata":{}},{"cell_type":"code","source":"import os\nfrom langchain_groq import ChatGroq\nfrom langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n\nllm = ChatGroq(model = \"llama-3.1-8b-instant\", groq_api_key = secret_value_0)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T21:50:32.700166Z","iopub.execute_input":"2024-08-08T21:50:32.700615Z","iopub.status.idle":"2024-08-08T21:50:32.733520Z","shell.execute_reply.started":"2024-08-08T21:50:32.700580Z","shell.execute_reply":"2024-08-08T21:50:32.732341Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"!pip install langchain_community","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-08-08T21:50:32.734783Z","iopub.execute_input":"2024-08-08T21:50:32.735088Z","iopub.status.idle":"2024-08-08T21:50:52.882950Z","shell.execute_reply.started":"2024-08-08T21:50:32.735061Z","shell.execute_reply":"2024-08-08T21:50:52.881603Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Collecting langchain_community\n  Downloading langchain_community-0.2.11-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (3.9.1)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (0.6.7)\nCollecting langchain<0.3.0,>=0.2.12 (from langchain_community)\n  Downloading langchain-0.2.12-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: langchain-core<0.3.0,>=0.2.27 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (0.2.29)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (0.1.98)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (1.26.4)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (8.2.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.3)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\nCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain<0.3.0,>=0.2.12->langchain_community)\n  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain<0.3.0,>=0.2.12->langchain_community) (2.5.3)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.27->langchain_community) (1.33)\nRequirement already satisfied: packaging<25,>=23.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.27->langchain_community) (24.1)\nRequirement already satisfied: typing-extensions>=4.7 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.27->langchain_community) (4.9.0)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (2024.7.4)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.27->langchain_community) (2.4)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.12->langchain_community) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.12->langchain_community) (2.14.6)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\nDownloading langchain_community-0.2.11-py3-none-any.whl (2.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain-0.2.12-py3-none-any.whl (990 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.6/990.6 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\nInstalling collected packages: langchain-text-splitters, langchain, langchain_community\nSuccessfully installed langchain-0.2.12 langchain-text-splitters-0.2.2 langchain_community-0.2.11\n","output_type":"stream"}]},{"cell_type":"markdown","source":"1. **`langchain_community.chat_message_histories.ChatMessageHistory`**:\n   - **Purpose**: This class manages the history of chat messages. \n   - **Functionality**: It allows for storing, retrieving, and managing a sequence of chat messages in a conversational AI or chatbot application. This history can be used to maintain context in ongoing conversations, enabling the chatbot to provide more relevant and coherent responses based on past interactions.\n\n2. **`langchain_core.chat_history.BaseChatMessageHistory`**:\n   - **Purpose**: This is a base or abstract class for chat message history.\n   - **Functionality**: It provides a foundational structure for implementing chat message history. Being a base class, it defines the basic interface and functionalities that any specific chat message history class should implement. This could include methods for adding messages, retrieving past messages, and possibly persisting the chat history to a storage medium.\n\n3. **`langchain_core.runnables.history.RunnableWithMessageHistory`**:\n   - **Purpose**: This class likely represents a runnable task or operation that includes or interacts with message history.\n   - **Functionality**: It combines the concept of a runnable task (something that can be executed) with message history. This means it can perform operations that involve or are influenced by the sequence of messages in the chat history. This could be useful for tasks that need to take into account previous conversations, such as generating responses, summarizing chats, or performing analytics on the conversation data.","metadata":{}},{"cell_type":"code","source":"# Importing library\nfrom langchain_community.chat_message_histories import ChatMessageHistory\nfrom langchain_core.chat_history import BaseChatMessageHistory\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\n\n# Dictionary to store chat histories for different sessions\nstore = {}\n\n# Function to get the chat history for a specific session\ndef get_session_history(session_id: str) -> BaseChatMessageHistory:\n    # If the session_id is not already in the store, create a new ChatMessageHistory\n    if session_id not in store:\n        store[session_id] = ChatMessageHistory()\n    # Return the chat history for the given session_id\n    return store[session_id]\n\nwith_message_history = RunnableWithMessageHistory(llm, get_session_history)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T21:50:52.884589Z","iopub.execute_input":"2024-08-08T21:50:52.884944Z","iopub.status.idle":"2024-08-08T21:50:52.925396Z","shell.execute_reply.started":"2024-08-08T21:50:52.884910Z","shell.execute_reply":"2024-08-08T21:50:52.924274Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"config = {\"configurable\":{\"session_id\":\"chat1\"}}","metadata":{"execution":{"iopub.status.busy":"2024-08-08T21:50:52.926855Z","iopub.execute_input":"2024-08-08T21:50:52.927245Z","iopub.status.idle":"2024-08-08T21:50:52.933171Z","shell.execute_reply.started":"2024-08-08T21:50:52.927213Z","shell.execute_reply":"2024-08-08T21:50:52.931778Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"response = with_message_history.invoke(\n    [HumanMessage(content = \"Hello my name is Aman and i am a Student\")],\n    config = config\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T21:50:52.934867Z","iopub.execute_input":"2024-08-08T21:50:52.935275Z","iopub.status.idle":"2024-08-08T21:50:53.326099Z","shell.execute_reply.started":"2024-08-08T21:50:52.935234Z","shell.execute_reply":"2024-08-08T21:50:53.324732Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"response.content","metadata":{"execution":{"iopub.status.busy":"2024-08-08T21:50:53.328187Z","iopub.execute_input":"2024-08-08T21:50:53.328622Z","iopub.status.idle":"2024-08-08T21:50:53.337211Z","shell.execute_reply.started":"2024-08-08T21:50:53.328587Z","shell.execute_reply":"2024-08-08T21:50:53.335833Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"'Hello Aman! Nice to meet you! What brings you here? Are you looking for help with your studies, or just chatting to pass the time?'"},"metadata":{}}]},{"cell_type":"markdown","source":"It remebers my name because i used same config","metadata":{}},{"cell_type":"code","source":"with_message_history.invoke(\n    [HumanMessage(content = \"What is my name?\")],\n    config = config\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T21:50:53.338983Z","iopub.execute_input":"2024-08-08T21:50:53.339492Z","iopub.status.idle":"2024-08-08T21:50:53.496789Z","shell.execute_reply.started":"2024-08-08T21:50:53.339437Z","shell.execute_reply":"2024-08-08T21:50:53.495486Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"AIMessage(content='Your name is Aman. You told me that earlier!', response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 67, 'total_tokens': 80, 'completion_time': 0.017333333, 'prompt_time': 0.021290091, 'queue_time': None, 'total_time': 0.038623424}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_9cb648b966', 'finish_reason': 'stop', 'logprobs': None}, id='run-951a74a2-f392-4a4b-8074-88f4e7b2dac3-0', usage_metadata={'input_tokens': 67, 'output_tokens': 13, 'total_tokens': 80})"},"metadata":{}}]},{"cell_type":"markdown","source":"After changing changing the config(session) llm don't know my name","metadata":{}},{"cell_type":"code","source":"# Changing the config\nconfig = {\"configurable\":{\"session_id\":\"chat2\"}}\n\nresponse = with_message_history.invoke(\n    [HumanMessage(content = \"What is my name?\")],\n    config = config\n)\nresponse.content","metadata":{"execution":{"iopub.status.busy":"2024-08-08T21:50:53.498255Z","iopub.execute_input":"2024-08-08T21:50:53.498691Z","iopub.status.idle":"2024-08-08T21:50:53.700121Z","shell.execute_reply.started":"2024-08-08T21:50:53.498659Z","shell.execute_reply":"2024-08-08T21:50:53.698949Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"\"I'm happy to chat with you, but I don't actually know your name. This is the beginning of our conversation, and I don't have any prior information about you. Would you like to introduce yourself?\""},"metadata":{}}]},{"cell_type":"markdown","source":"# Prompt Templates\n\nPrompt Templates help to turn raw user information into a format that the LLM can work with. In this case the raw user input in just a messag, which we are passing to the LLM. Let's now make that a bit more complicated.\nFirst let's add in a System Message with some custom instrutions (but still taking message as input.) Next we'll add in more input besides just the messages. \n\nWe use `ChatPromptTemplate` for several important reasons:\n\n1. **Standardizing Inputs**: It helps convert raw user inputs into a format that the language model can work with effectively. This ensures consistency in how information is presented to the model.\n\n2. **Adding Context and Instructions**: It allows us to include system messages or custom instructions along with user messages. This helps guide the language model on how to respond appropriately.\n\n3. **Flexibility and Reusability**: `ChatPromptTemplate` makes it easier to create and reuse prompt structures across different scenarios, saving time and ensuring that best practices are followed.\n\n4. **Simplifying Complex Interactions**: When dealing with complex interactions or multiple variables (like different types of messages or additional parameters), `ChatPromptTemplate` helps manage these elements in a clear and organized way.\n\n5. **Error Handling and Debugging**: By using a structured template, it's easier to identify and fix errors in how prompts are constructed and processed, leading to more reliable and accurate interactions with the language model.\n\nIn summary, `ChatPromptTemplate` helps in creating clear, structured, and effective prompts that improve the quality of interactions with the language model.\n\nWe use `MessagesPlaceholder` for a few key reasons:\n\n1. **Organizing User Input**: It helps turn user messages into a structured format that the language model can understand better.\n  \n2. **Adding Instructions**: It allows us to add extra instructions or context to the messages, like telling the model it's a helpful assistant.\n\n3. **Handling Different Messages**: It helps separate and manage different types of messages (like system messages and user messages).\n\n4. **Managing Multiple Inputs**: It allows us to handle additional information (like specifying a language) along with the user messages.\n\nOverall, `MessagesPlaceholder` makes sure the messages are clear and correctly formatted for the language model to process.","metadata":{}},{"cell_type":"code","source":"from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are a Helpful Assistant. Answer the Questions to the Best of your Ability\"),\n        MessagesPlaceholder(variable_name=\"messages\")\n    ]\n)\n\nchain = prompt | llm","metadata":{"execution":{"iopub.status.busy":"2024-08-08T21:50:53.701858Z","iopub.execute_input":"2024-08-08T21:50:53.702355Z","iopub.status.idle":"2024-08-08T21:50:53.709311Z","shell.execute_reply.started":"2024-08-08T21:50:53.702311Z","shell.execute_reply":"2024-08-08T21:50:53.707766Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"prompt","metadata":{"execution":{"iopub.status.busy":"2024-08-08T21:50:53.710975Z","iopub.execute_input":"2024-08-08T21:50:53.711411Z","iopub.status.idle":"2024-08-08T21:50:53.724391Z","shell.execute_reply.started":"2024-08-08T21:50:53.711379Z","shell.execute_reply":"2024-08-08T21:50:53.723184Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"ChatPromptTemplate(input_variables=['messages'], input_types={'messages': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a Helpful Assistant. Answer the Questions to the Best of your Ability')), MessagesPlaceholder(variable_name='messages')])"},"metadata":{}}]},{"cell_type":"code","source":"response = chain.invoke({\"messages\":[HumanMessage(content = \"Hi My name is Ramesh\")]})\nresponse.content","metadata":{"execution":{"iopub.status.busy":"2024-08-08T21:50:53.726420Z","iopub.execute_input":"2024-08-08T21:50:53.726893Z","iopub.status.idle":"2024-08-08T21:50:53.858838Z","shell.execute_reply.started":"2024-08-08T21:50:53.726838Z","shell.execute_reply":"2024-08-08T21:50:53.857563Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"\"Hello Ramesh! It's nice to meet you. How can I assist you today?\""},"metadata":{}}]},{"cell_type":"code","source":"with_message_history = RunnableWithMessageHistory(chain, get_session_history)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T21:50:53.860503Z","iopub.execute_input":"2024-08-08T21:50:53.860903Z","iopub.status.idle":"2024-08-08T21:50:53.867485Z","shell.execute_reply.started":"2024-08-08T21:50:53.860863Z","shell.execute_reply":"2024-08-08T21:50:53.866131Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"config = {\"configurable\": {\"session_id\": \"chat3\"}}\nresponse = with_message_history.invoke([HumanMessage(content = \"Hi my name is Aadam\")],\n                                      config=config\n                                      )\nresponse.content","metadata":{"execution":{"iopub.status.busy":"2024-08-08T21:50:53.869240Z","iopub.execute_input":"2024-08-08T21:50:53.869761Z","iopub.status.idle":"2024-08-08T21:50:54.071805Z","shell.execute_reply.started":"2024-08-08T21:50:53.869719Z","shell.execute_reply":"2024-08-08T21:50:54.070566Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"\"Nice to meet you, Aadam! It's great to have you here. Is there something I can help you with today?\""},"metadata":{}}]},{"cell_type":"code","source":"from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are a Helpful Assistant. Answer the Questions to the Best of your Ability in {language}\"),\n        MessagesPlaceholder(variable_name=\"messages\")\n    ]\n)\n\nchain = prompt | llm","metadata":{"execution":{"iopub.status.busy":"2024-08-08T21:50:54.073392Z","iopub.execute_input":"2024-08-08T21:50:54.073811Z","iopub.status.idle":"2024-08-08T21:50:54.080910Z","shell.execute_reply.started":"2024-08-08T21:50:54.073777Z","shell.execute_reply":"2024-08-08T21:50:54.079508Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"response = chain.invoke({\"messages\": [HumanMessage(content = \"Hello my name is Esha\")], \"language\":\"Hindi\"})\nresponse.content","metadata":{"execution":{"iopub.status.busy":"2024-08-08T21:50:54.082841Z","iopub.execute_input":"2024-08-08T21:50:54.083236Z","iopub.status.idle":"2024-08-08T21:50:54.258677Z","shell.execute_reply.started":"2024-08-08T21:50:54.083205Z","shell.execute_reply":"2024-08-08T21:50:54.257379Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"'नमस्ते! मैं आपका मददगार सहायक हूँ। आपका नाम ईशा है, ना? क्या मैं आपकी किसी भी चीज़ में मदद कर सकता हूँ?'"},"metadata":{}}]},{"cell_type":"markdown","source":"Let's now wrap this more complex chain in a Message History class. This time because there are multiple keys in the input we need to specify the correct key to use to save the chat history","metadata":{}},{"cell_type":"code","source":"with_message_history = RunnableWithMessageHistory(chain, \n                                                  get_session_history,\n                                                 input_messages_key = \"messages\"\n                                                 )","metadata":{"execution":{"iopub.status.busy":"2024-08-08T21:50:54.260088Z","iopub.execute_input":"2024-08-08T21:50:54.260488Z","iopub.status.idle":"2024-08-08T21:50:54.266879Z","shell.execute_reply.started":"2024-08-08T21:50:54.260414Z","shell.execute_reply":"2024-08-08T21:50:54.265725Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"config = {\"configurable\": {\"session_id\": \"chat4\"}}\n\nresponse = with_message_history.invoke({\"messages\":[HumanMessage(content = \"Hello, My name is Raftaar\")],\n                                       \"language\":\"Marathi\"},\n                                      config=config\n                                      )\nresponse.content","metadata":{"execution":{"iopub.status.busy":"2024-08-08T21:50:54.268224Z","iopub.execute_input":"2024-08-08T21:50:54.268635Z","iopub.status.idle":"2024-08-08T21:50:54.552678Z","shell.execute_reply.started":"2024-08-08T21:50:54.268603Z","shell.execute_reply":"2024-08-08T21:50:54.551449Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"'नमस्कार राफ्तार! तुमचे मी मदतीला असणार्\\u200dया सहाय्यक म्हणून येऊन होतो. काय गोष्टी असू शकतात ज्यामध्ये मी तुम्हाला मदत करू शकतो?'"},"metadata":{}}]},{"cell_type":"code","source":"response = with_message_history.invoke({\"messages\":[HumanMessage(content = \"What is my name?\")],\n                                       \"language\":\"Marathi\"},\n                                      config=config\n                                      )\nresponse.content","metadata":{"execution":{"iopub.status.busy":"2024-08-08T21:50:54.554324Z","iopub.execute_input":"2024-08-08T21:50:54.554790Z","iopub.status.idle":"2024-08-08T21:50:54.784192Z","shell.execute_reply.started":"2024-08-08T21:50:54.554751Z","shell.execute_reply":"2024-08-08T21:50:54.782962Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"'तुझे नाव राफ्तार आहे!'"},"metadata":{}}]},{"cell_type":"markdown","source":"# Managing the Conversation History\n\nOne important concept to understand when building chatbot is how to manage conversation history.If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM.Therefore, it is important to add a step that limits the size of the messages that you are passing in.\n\nAll models have finite context windows, meaning there's a limit to how many tokens they can take as input. If you have very long messages or a chain/agent that accumulates a long message is history, you'll need to manage the length of the messages you're passing in to the model.\n\nThe `trim_messages` util provides some basic strategies for trimming a list of messages to be of a certain token length.\n\n[How to trim messages](https://python.langchain.com/v0.2/docs/how_to/trim_messages/#:~:text=If%20you%20have%20very%20long,of%20a%20certain%20token%20length.)","metadata":{}},{"cell_type":"code","source":"from langchain_core.messages import SystemMessage, trim_messages\n\n# Initialize the trimmer with specific parameters\ntrimmer = trim_messages(\n    max_tokens=70,               # The maximum number of tokens allowed after trimming.\n    strategy=\"last\",            # The strategy used to trim messages; \"last\" means it will remove messages from the end until the token limit is met.\n    token_counter=llm,          # The object or function used to count tokens; here, 'llm' likely refers to a language model that counts tokens.\n    include_system=True,        # Whether to include system messages (like instructions) in the trimming process.\n    allow_partial=True,         # Whether partial messages can be included when trimming.\n    start_on=\"human\",           # Specifies to start trimming after a \"human\" message is encountered.\n)\n\nmessages = [\n    SystemMessage(content=\"you're a good assistant\"),\n    HumanMessage(content=\"hi! I'm bob\"),\n    AIMessage(content=\"hi!\"),\n    HumanMessage(content=\"I like vanilla ice cream\"),\n    AIMessage(content=\"nice\"),\n    HumanMessage(content=\"whats 2 + 2\"),\n    AIMessage(content=\"4\"),\n    HumanMessage(content=\"thanks\"),\n    AIMessage(content=\"no problem!\"),\n    HumanMessage(content=\"having fun?\"),\n    AIMessage(content=\"yes!\"),\n]\ntrimmer.invoke(messages)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T22:10:59.639410Z","iopub.execute_input":"2024-08-08T22:10:59.639871Z","iopub.status.idle":"2024-08-08T22:10:59.656304Z","shell.execute_reply.started":"2024-08-08T22:10:59.639838Z","shell.execute_reply":"2024-08-08T22:10:59.654924Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"[SystemMessage(content=\"you're a good assistant\"),\n HumanMessage(content=\"hi! I'm bob\"),\n AIMessage(content='hi!'),\n HumanMessage(content='I like vanilla ice cream'),\n AIMessage(content='nice'),\n HumanMessage(content='whats 2 + 2'),\n AIMessage(content='4'),\n HumanMessage(content='thanks'),\n AIMessage(content='no problem!'),\n HumanMessage(content='having fun?'),\n AIMessage(content='yes!')]"},"metadata":{}}]},{"cell_type":"markdown","source":"`itemgetter` is a utility function that creates a callable object (essentially a function) that can retrieve specific items from an iterable, such as lists, tuples, or dictionaries. It's often used in sorting operations, particularly when you need to sort a list of tuples or dictionaries by one or more keys.\n\n`RunnablePassthrough` in LangChain is like a \"bypass\" in a pipeline. It lets data flow through without changing it. Think of it as a step in a process that doesn't do anything but just passes the data along to the next step. This is useful if you want to keep the process going without altering the data at certain points.","metadata":{}},{"cell_type":"code","source":"from operator import itemgetter\nfrom langchain_core.runnables import RunnablePassthrough\n\nchain=(\n    RunnablePassthrough.assign(messages=itemgetter(\"messages\")|trimmer)\n    | prompt\n    | llm\n    \n)\n\nresponse=chain.invoke(\n    {\n    \"messages\":messages + [HumanMessage(content=\"What ice cream do i like\")],\n    \"language\":\"English\"\n    }\n)\nresponse.content","metadata":{"execution":{"iopub.status.busy":"2024-08-08T22:14:57.676478Z","iopub.execute_input":"2024-08-08T22:14:57.677490Z","iopub.status.idle":"2024-08-08T22:14:57.872246Z","shell.execute_reply.started":"2024-08-08T22:14:57.677429Z","shell.execute_reply":"2024-08-08T22:14:57.871041Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"'You like vanilla ice cream!'"},"metadata":{}}]},{"cell_type":"code","source":"response = chain.invoke(\n    {\n        \"messages\": messages + [HumanMessage(content=\"what math problem did i ask\")],\n        \"language\": \"English\",\n    }\n)\nresponse.content","metadata":{"execution":{"iopub.status.busy":"2024-08-08T22:13:18.370670Z","iopub.execute_input":"2024-08-08T22:13:18.371186Z","iopub.status.idle":"2024-08-08T22:13:18.565109Z","shell.execute_reply.started":"2024-08-08T22:13:18.371147Z","shell.execute_reply":"2024-08-08T22:13:18.563779Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"'You asked me \"whats 2 + 2\"'"},"metadata":{}}]},{"cell_type":"code","source":"## Lets wrap this in the Message History\n\nwith_message_history = RunnableWithMessageHistory(\n    chain,\n    get_session_history,\n    input_messages_key=\"messages\",\n)\nconfig={\"configurable\":{\"session_id\":\"chat5\"}}","metadata":{"execution":{"iopub.status.busy":"2024-08-08T22:16:16.722804Z","iopub.execute_input":"2024-08-08T22:16:16.723971Z","iopub.status.idle":"2024-08-08T22:16:16.730878Z","shell.execute_reply.started":"2024-08-08T22:16:16.723930Z","shell.execute_reply":"2024-08-08T22:16:16.729661Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"response = with_message_history.invoke(\n    {\n        \"messages\": messages + [HumanMessage(content=\"you are a?\")],\n        \"language\": \"English\",\n    },\n    config=config,\n)\n\nresponse.content","metadata":{"execution":{"iopub.status.busy":"2024-08-08T22:17:13.762789Z","iopub.execute_input":"2024-08-08T22:17:13.763235Z","iopub.status.idle":"2024-08-08T22:17:14.115895Z","shell.execute_reply.started":"2024-08-08T22:17:13.763200Z","shell.execute_reply":"2024-08-08T22:17:14.114592Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"\"I'm a Helpful Assistant!\""},"metadata":{}}]}]}